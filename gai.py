# -*- coding: utf-8 -*-
"""GAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10rf8xuXES-0Xkt6bfwVhQT7X8C8Yhiy_
"""

import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support
import random
import time
import matplotlib.pyplot as plt

# Load and preprocess data
iris = load_iris()
X = iris.data
y = iris.target.reshape(-1, 1)

scaler = StandardScaler()
X = scaler.fit_transform(X)

encoder = OneHotEncoder(sparse_output=False)
y_onehot = encoder.fit_transform(y)

# Split into train, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y_onehot, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# MLP architecture
input_size = 4
hidden_size = 5
output_size = 3

# Calculate total weights for GA
def get_num_weights():
    return (input_size * hidden_size) + hidden_size + (hidden_size * output_size) + output_size

# Decode genome into weights and biases
def decode_weights(genome):
    idx = 0
    W1 = genome[idx:idx + input_size * hidden_size].reshape((input_size, hidden_size))
    idx += input_size * hidden_size
    b1 = genome[idx:idx + hidden_size]
    idx += hidden_size
    W2 = genome[idx:idx + hidden_size * output_size].reshape((hidden_size, output_size))
    idx += hidden_size * output_size
    b2 = genome[idx:idx + output_size]
    return W1, b1, W2, b2

# Activation functions
def relu(x): return np.maximum(0, x)

def softmax(x):
    # Numerically stable version
    shifted_x = x - np.max(x, axis=1, keepdims=True)
    exp_x = np.exp(shifted_x)
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)

# Forward pass using genome
def forward_pass(genome, X):
    W1, b1, W2, b2 = decode_weights(genome)
    z1 = relu(np.dot(X, W1) + b1)
    output = softmax(np.dot(z1, W2) + b2)
    return output

# Evaluate model on data
def evaluate(genome, X, y):
    preds = forward_pass(genome, X)
    pred_labels = np.argmax(preds, axis=1)
    true_labels = np.argmax(y, axis=1)

    # Check if all classes are being predicted
    unique_preds = set(pred_labels)
    unique_true = set(true_labels)

    if len(unique_preds) < len(unique_true):
        print(f"Warning: Model only predicting {len(unique_preds)} out of {len(unique_true)} classes")

    acc = accuracy_score(true_labels, pred_labels)

    # Use zero_division parameter
    precision, recall, f1, _ = precision_recall_fscore_support(
        true_labels, pred_labels, average='weighted', zero_division=0
    )

    return {
        'accuracy': acc,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'predictions': pred_labels,
        'true_labels': true_labels
    }

# Fitness = Accuracy on training data
def fitness(genome):
    eval_results = evaluate(genome, X_train, y_train)
    return eval_results['accuracy']

# GA Parameters
POP_SIZE = 30
GENOME_LENGTH = get_num_weights()
GENERATIONS = 100
MUTATION_RATE = 0.1
CONVERGENCE_THRESHOLD = 0.01  # Define convergence as <1% improvement over 10 generations

# Performance metrics tracking
train_accuracy_history = []
val_accuracy_history = []
best_fitness_history = []
avg_fitness_history = []
execution_times = []
convergence_generation = None

# Initialize population
population = [np.random.uniform(-1, 1, GENOME_LENGTH) for _ in range(POP_SIZE)]

# Start timer for total execution
start_time_total = time.time()

# GA main loop
for gen in range(GENERATIONS):
    # Start timer for this generation
    start_time_gen = time.time()

    # Evaluate fitness
    fitness_scores = [fitness(ind) for ind in population]

    # Record best and average fitness
    best_fitness = max(fitness_scores)
    avg_fitness = sum(fitness_scores) / len(fitness_scores)
    best_fitness_history.append(best_fitness)
    avg_fitness_history.append(avg_fitness)

    # Get best genome for this generation
    best_genome = population[np.argmax(fitness_scores)]

    # Evaluate on train and validation sets
    train_eval = evaluate(best_genome, X_train, y_train)
    val_eval = evaluate(best_genome, X_val, y_val)

    train_accuracy_history.append(train_eval['accuracy'])
    val_accuracy_history.append(val_eval['accuracy'])

    # Selection (Tournament)
    selected = []
    for _ in range(POP_SIZE):
        a, b = random.sample(range(POP_SIZE), 2)
        winner = population[a] if fitness_scores[a] > fitness_scores[b] else population[b]
        selected.append(winner)

    # Crossover (Single-point)
    next_population = []
    for i in range(0, POP_SIZE, 2):
        p1, p2 = selected[i], selected[min(i+1, POP_SIZE-1)]
        point = random.randint(1, GENOME_LENGTH - 1)
        child1 = np.concatenate((p1[:point], p2[point:]))
        child2 = np.concatenate((p2[:point], p1[point:]))
        next_population.extend([child1, child2])

    # Ensure population size is maintained
    next_population = next_population[:POP_SIZE]

    # Mutation
    for i in range(POP_SIZE):
        if random.random() < MUTATION_RATE:
            mut_point = random.randint(0, GENOME_LENGTH - 1)
            next_population[i][mut_point] += np.random.normal()

    population = next_population

    # Record execution time for this generation
    end_time_gen = time.time()
    execution_times.append(end_time_gen - start_time_gen)

    # Check for convergence
    if gen >= 10 and convergence_generation is None:
        improvement = (best_fitness_history[-1] - best_fitness_history[-10]) / best_fitness_history[-10]
        if improvement < CONVERGENCE_THRESHOLD:
            convergence_generation = gen
            print(f"Converged at generation {gen} with accuracy {best_fitness:.4f}")

    # Print progress and metrics
    if gen % 10 == 0 or gen == GENERATIONS - 1:
        print(f"Generation {gen} - Train Acc: {train_eval['accuracy']:.4f}, Val Acc: {val_eval['accuracy']:.4f}, Time: {execution_times[-1]:.4f}s")
        # Print overfitting metric (train acc - val acc)
        overfitting_gap = train_eval['accuracy'] - val_eval['accuracy']
        print(f"Overfitting gap: {overfitting_gap:.4f}")

# End timer for total execution
end_time_total = time.time()
total_time = end_time_total - start_time_total

# Evaluate on test data using best genome
final_best_genome = population[np.argmax([fitness(ind) for ind in population])]
final_train_eval = evaluate(final_best_genome, X_train, y_train)
final_val_eval = evaluate(final_best_genome, X_val, y_val)
final_test_eval = evaluate(final_best_genome, X_test, y_test)

# Print final results
print("\n======= FINAL RESULTS =======")
print(f"Total execution time: {total_time:.4f} seconds")
print(f"Average time per generation: {sum(execution_times)/len(execution_times):.4f} seconds")

if convergence_generation:
    print(f"Converged at generation: {convergence_generation}")
else:
    print("Did not converge within threshold")

print(f"\nFinal Train Accuracy: {final_train_eval['accuracy']:.4f}")
print(f"Final Validation Accuracy: {final_val_eval['accuracy']:.4f}")
print(f"Final Test Accuracy: {final_test_eval['accuracy']:.4f}")

# Overfitting analysis
train_val_gap = final_train_eval['accuracy'] - final_val_eval['accuracy']
print(f"\nOverfitting Analysis:")
print(f"Train-Validation accuracy gap: {train_val_gap:.4f}")

if train_val_gap > 0.1:
    print("WARNING: Possible overfitting detected (train accuracy significantly higher than validation)")
elif final_train_eval['accuracy'] < 0.7:
    print("WARNING: Possible underfitting detected (train accuracy is low)")
else:
    print("Model fit appears reasonable")

# Plot metrics
plt.figure(figsize=(15, 10))

# Plot 1: Accuracy over generations
plt.subplot(2, 2, 1)
plt.plot(train_accuracy_history, label='Train Accuracy')
plt.plot(val_accuracy_history, label='Validation Accuracy')
plt.title('Accuracy over Generations')
plt.xlabel('Generation')
plt.ylabel('Accuracy')
plt.legend()

# Plot 2: Convergence (best and average fitness)
plt.subplot(2, 2, 2)
plt.plot(best_fitness_history, label='Best Fitness')
plt.plot(avg_fitness_history, label='Average Fitness')
plt.title('Fitness Convergence')
plt.xlabel('Generation')
plt.ylabel('Fitness')
plt.legend()

# Plot 3: Execution time per generation
plt.subplot(2, 2, 3)
plt.plot(execution_times)
plt.title('Execution Time per Generation')
plt.xlabel('Generation')
plt.ylabel('Time (seconds)')

# Plot 4: Overfitting gap over generations
plt.subplot(2, 2, 4)
overfitting_gaps = [t - v for t, v in zip(train_accuracy_history, val_accuracy_history)]
plt.plot(overfitting_gaps)
plt.axhline(y=0.1, color='r', linestyle='--', label='Overfitting Threshold')
plt.title('Overfitting Gap (Train - Validation)')
plt.xlabel('Generation')
plt.ylabel('Accuracy Gap')

plt.tight_layout()
plt.savefig('ga_performance_metrics.png')
plt.show()

# Confusion matrix for the test set
cm = confusion_matrix(final_test_eval['true_labels'], final_test_eval['predictions'])

# Plot confusion matrix
plt.figure(figsize=(8, 6))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()
tick_marks = np.arange(len(iris.target_names))
plt.xticks(tick_marks, iris.target_names, rotation=45)
plt.yticks(tick_marks, iris.target_names)
plt.xlabel('Predicted label')
plt.ylabel('True label')

# Add text annotations to confusion matrix cells
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, cm[i, j], horizontalalignment="center", color="white" if cm[i, j] > cm.max() / 2 else "black")

plt.tight_layout()
plt.savefig('confusion_matrix.png')
plt.show()