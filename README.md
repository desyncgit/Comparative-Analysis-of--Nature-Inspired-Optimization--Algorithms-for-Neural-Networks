# Comparative Analysis of  Nature-Inspired Optimization  Algorithms for Neural Networks
This project presents a comprehensive comparative study of three popular nature-inspired optimization algorithms—Genetic Algorithm (GA), Ant Colony Optimization (ACO), and Artificial Bee Colony (ABC)—for training a Multilayer Perceptron (MLP) neural network model.
The experiments are performed using the Iris dataset, a well-known classification benchmark, to evaluate the training efficiency, convergence behavior, and generalization ability of each algorithm.

# Key Objectives
Apply GA, ACO, and ABC algorithms to optimize the weights and biases of an MLP model.

Compare their training performance based on accuracy, precision, recall, F1-score, convergence speed, and runtime.

Identify the strengths and weaknesses of each algorithm in neural network optimization.

# Methodology
Dataset: Iris Dataset (features normalized, 80/20 train-test split).

Model: Multilayer Perceptron (one hidden layer, fixed architecture).

Optimization Techniques:

GA: Evolutionary search using selection, crossover, and mutation.

ACO: Probabilistic construction of solutions guided by pheromone trails.

ABC: Foraging behavior of bees simulated to explore and exploit weight space.

Evaluation Metrics: Train accuracy, validation accuracy, test accuracy, precision, recall, F1-score.

# Acknowledgements
This project was guided and supervised as part of an academic research initiative under Chandigarh University.


