# Comparative Analysis of  Nature-Inspired Optimization  Algorithms for Neural Networks
This project presents a comprehensive comparative study of three popular nature-inspired optimization algorithms—Genetic Algorithm (GA), Ant Colony Optimization (ACO), and Artificial Bee Colony (ABC)—for training a Multilayer Perceptron (MLP) neural network model.
The experiments are performed using the Iris dataset, a well-known classification benchmark, to evaluate the training efficiency, convergence behavior, and generalization ability of each algorithm.

# Key Objectives
Apply GA, ACO, and ABC algorithms to optimize the weights and biases of an MLP model.

Compare their training performance based on accuracy, precision, recall, F1-score, convergence speed, and runtime.

Identify the strengths and weaknesses of each algorithm in neural network optimization.


