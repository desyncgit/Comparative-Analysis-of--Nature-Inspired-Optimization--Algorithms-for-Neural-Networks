# -*- coding: utf-8 -*-
"""ABCI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1otlZ-VFTW5Q1dA-3Kzw7esRZxxyW-VCY
"""

import numpy as np
from random import choice
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support
import time
import matplotlib.pyplot as plt

# Load and preprocess data
iris = load_iris()
X = iris.data
y = iris.target.reshape(-1, 1)

scaler = StandardScaler()
X = scaler.fit_transform(X)

encoder = OneHotEncoder(sparse_output=False)
y_onehot = encoder.fit_transform(y)

# Split into train, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y_onehot, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# MLP architecture
input_size = 4
hidden_size = 5
output_size = 3

def get_num_weights():
    return (input_size * hidden_size) + hidden_size + (hidden_size * output_size) + output_size

GENOME_LENGTH = get_num_weights()

# Activation functions
def relu(x): return np.maximum(0, x)

def softmax(x):
    try:
        # Shift values by the maximum of each row for numerical stability
        shifted_x = x - np.max(x, axis=1, keepdims=True)
        exp_x = np.exp(shifted_x)
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)
    except:
        # Handle edge cases (like single sample input)
        if x.ndim == 1:
            shifted_x = x - np.max(x)
            exp_x = np.exp(shifted_x)
            return exp_x / np.sum(exp_x)
        raise

def decode_weights(genome):
    idx = 0
    W1 = genome[idx:idx + input_size * hidden_size].reshape((input_size, hidden_size))
    idx += input_size * hidden_size
    b1 = genome[idx:idx + hidden_size]
    idx += hidden_size
    W2 = genome[idx:idx + hidden_size * output_size].reshape((hidden_size, output_size))
    idx += hidden_size * output_size
    b2 = genome[idx:idx + output_size]
    return W1, b1, W2, b2

def forward_pass(genome, X):
    W1, b1, W2, b2 = decode_weights(genome)
    z1 = relu(np.dot(X, W1) + b1)
    output = softmax(np.dot(z1, W2) + b2)
    return output

# Evaluate model on data with comprehensive metrics
def evaluate(genome, X, y):
    preds = forward_pass(genome, X)
    pred_labels = np.argmax(preds, axis=1)
    true_labels = np.argmax(y, axis=1)

    # Check if all classes are being predicted
    unique_preds = set(pred_labels)
    unique_true = set(true_labels)

    if len(unique_preds) < len(unique_true):
        print(f"Warning: Model only predicting {len(unique_preds)} out of {len(unique_true)} classes")

    acc = accuracy_score(true_labels, pred_labels)

    # Use zero_division parameter to avoid warnings
    precision, recall, f1, _ = precision_recall_fscore_support(
        true_labels, pred_labels, average='weighted', zero_division=0
    )

    return {
        'accuracy': acc,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'predictions': pred_labels,
        'true_labels': true_labels
    }

def fitness(genome):
    eval_results = evaluate(genome, X_train, y_train)
    return eval_results['accuracy']

# ABC parameters
NUM_BEES = 30
LIMIT = 20
ITERATIONS = 100
CONVERGENCE_THRESHOLD = 0.01  # Define convergence as <1% improvement over 10 iterations

# Performance metrics tracking
train_accuracy_history = []
val_accuracy_history = []
best_fitness_history = []
avg_fitness_history = []
execution_times = []
convergence_iteration = None

# Initialize population
solutions = [np.random.uniform(-1, 1, GENOME_LENGTH) for _ in range(NUM_BEES)]
fitness_scores = [fitness(sol) for sol in solutions]
trial_counters = [0 for _ in range(NUM_BEES)]

# Start timer for total execution
start_time_total = time.time()

for itr in range(ITERATIONS):
    # Start timer for this iteration
    start_time_iter = time.time()

    # Employed Bees Phase
    for i in range(NUM_BEES):
        k = choice([j for j in range(NUM_BEES) if j != i])
        phi = np.random.uniform(-1, 1, GENOME_LENGTH)
        new_solution = solutions[i] + phi * (solutions[i] - solutions[k])
        new_fitness = fitness(new_solution)
        if new_fitness > fitness_scores[i]:
            solutions[i] = new_solution
            fitness_scores[i] = new_fitness
            trial_counters[i] = 0
        else:
            trial_counters[i] += 1

    # Onlooker Bees Phase
    total_fit = sum(fitness_scores)
    if total_fit == 0:  # Avoid division by zero
        probs = [1/NUM_BEES for _ in range(NUM_BEES)]
    else:
        probs = [f / total_fit for f in fitness_scores]

    for _ in range(NUM_BEES):
        i = np.random.choice(range(NUM_BEES), p=probs)
        k = choice([j for j in range(NUM_BEES) if j != i])
        phi = np.random.uniform(-1, 1, GENOME_LENGTH)
        new_solution = solutions[i] + phi * (solutions[i] - solutions[k])
        new_fitness = fitness(new_solution)
        if new_fitness > fitness_scores[i]:
            solutions[i] = new_solution
            fitness_scores[i] = new_fitness
            trial_counters[i] = 0
        else:
            trial_counters[i] += 1

    # Scout Bees Phase
    for i in range(NUM_BEES):
        if trial_counters[i] > LIMIT:
            solutions[i] = np.random.uniform(-1, 1, GENOME_LENGTH)
            fitness_scores[i] = fitness(solutions[i])
            trial_counters[i] = 0

    # Record metrics for this iteration
    best_idx = np.argmax(fitness_scores)
    best_solution = solutions[best_idx]

    # Record best and average fitness
    best_fitness = max(fitness_scores)
    avg_fitness = sum(fitness_scores) / len(fitness_scores)
    best_fitness_history.append(best_fitness)
    avg_fitness_history.append(avg_fitness)

    # Evaluate on train and validation sets
    train_eval = evaluate(best_solution, X_train, y_train)
    val_eval = evaluate(best_solution, X_val, y_val)

    train_accuracy_history.append(train_eval['accuracy'])
    val_accuracy_history.append(val_eval['accuracy'])

    # Record execution time for this iteration
    end_time_iter = time.time()
    execution_times.append(end_time_iter - start_time_iter)

    # Check for convergence
    if itr >= 10 and convergence_iteration is None:
        improvement = (best_fitness_history[-1] - best_fitness_history[-10]) / best_fitness_history[-10] if best_fitness_history[-10] > 0 else float('inf')
        if improvement < CONVERGENCE_THRESHOLD:
            convergence_iteration = itr
            print(f"Converged at iteration {itr} with accuracy {best_fitness:.4f}")

    # Print progress and metrics
    if itr % 10 == 0 or itr == ITERATIONS - 1:
        print(f"Iteration {itr} - Train Acc: {train_eval['accuracy']:.4f}, Val Acc: {val_eval['accuracy']:.4f}, Time: {execution_times[-1]:.4f}s")
        # Print overfitting metric (train acc - val acc)
        overfitting_gap = train_eval['accuracy'] - val_eval['accuracy']
        print(f"Overfitting gap: {overfitting_gap:.4f}")

# End timer for total execution
end_time_total = time.time()
total_time = end_time_total - start_time_total

# Final evaluation on all datasets
best_idx = np.argmax(fitness_scores)
best_solution = solutions[best_idx]

final_train_eval = evaluate(best_solution, X_train, y_train)
final_val_eval = evaluate(best_solution, X_val, y_val)
final_test_eval = evaluate(best_solution, X_test, y_test)

# Print final results
print("\n======= FINAL RESULTS =======")
print(f"Total execution time: {total_time:.4f} seconds")
print(f"Average time per iteration: {sum(execution_times)/len(execution_times):.4f} seconds")

if convergence_iteration:
    print(f"Converged at iteration: {convergence_iteration}")
else:
    print("Did not converge within threshold")

print(f"\nFinal Train Accuracy: {final_train_eval['accuracy']:.4f}")
print(f"Final Validation Accuracy: {final_val_eval['accuracy']:.4f}")
print(f"Final Test Accuracy: {final_test_eval['accuracy']:.4f}")
print(f"Precision: {final_test_eval['precision']:.4f}")
print(f"Recall: {final_test_eval['recall']:.4f}")
print(f"F1 Score: {final_test_eval['f1']:.4f}")

# Overfitting analysis
train_val_gap = final_train_eval['accuracy'] - final_val_eval['accuracy']
print(f"\nOverfitting Analysis:")
print(f"Train-Validation accuracy gap: {train_val_gap:.4f}")

if train_val_gap > 0.1:
    print("WARNING: Possible overfitting detected (train accuracy significantly higher than validation)")
elif final_train_eval['accuracy'] < 0.7:
    print("WARNING: Possible underfitting detected (train accuracy is low)")
else:
    print("Model fit appears reasonable")

# Plot metrics
plt.figure(figsize=(15, 10))

# Plot 1: Accuracy over iterations
plt.subplot(2, 2, 1)
plt.plot(train_accuracy_history, label='Train Accuracy')
plt.plot(val_accuracy_history, label='Validation Accuracy')
plt.title('Accuracy over Iterations')
plt.xlabel('Iteration')
plt.ylabel('Accuracy')
plt.legend()

# Plot 2: Convergence (best and average fitness)
plt.subplot(2, 2, 2)
plt.plot(best_fitness_history, label='Best Fitness')
plt.plot(avg_fitness_history, label='Average Fitness')
plt.title('Fitness Convergence')
plt.xlabel('Iteration')
plt.ylabel('Fitness')
plt.legend()

# Plot 3: Execution time per iteration
plt.subplot(2, 2, 3)
plt.plot(execution_times)
plt.title('Execution Time per Iteration')
plt.xlabel('Iteration')
plt.ylabel('Time (seconds)')

# Plot 4: Overfitting gap over iterations
plt.subplot(2, 2, 4)
overfitting_gaps = [t - v for t, v in zip(train_accuracy_history, val_accuracy_history)]
plt.plot(overfitting_gaps)
plt.axhline(y=0.1, color='r', linestyle='--', label='Overfitting Threshold')
plt.title('Overfitting Gap (Train - Validation)')
plt.xlabel('Iteration')
plt.ylabel('Accuracy Gap')

plt.tight_layout()
plt.savefig('abc_performance_metrics.png')
plt.show()

# Confusion matrix for the test set
cm = confusion_matrix(final_test_eval['true_labels'], final_test_eval['predictions'])

# Plot confusion matrix
plt.figure(figsize=(8, 6))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()
tick_marks = np.arange(len(iris.target_names))
plt.xticks(tick_marks, iris.target_names, rotation=45)
plt.yticks(tick_marks, iris.target_names)
plt.xlabel('Predicted label')
plt.ylabel('True label')

# Add text annotations to confusion matrix cells
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, cm[i, j], horizontalalignment="center", color="white" if cm[i, j] > cm.max() / 2 else "black")

plt.tight_layout()
plt.savefig('abc_confusion_matrix.png')
plt.show()