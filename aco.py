# -*- coding: utf-8 -*-
"""ACO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sMjEdwP8QCcXsigcQHj11adn37QeWm0B
"""

import numpy as np
import random
from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import time
import matplotlib.pyplot as plt

# Load and preprocess data
iris = load_iris()
X = iris.data
y = iris.target.reshape(-1, 1)

scaler = StandardScaler()
X = scaler.fit_transform(X)

encoder = OneHotEncoder(sparse_output=False)
y_onehot = encoder.fit_transform(y)

# Split into train, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y_onehot, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# MLP setup
input_size = 4
hidden_size = 5
output_size = 3

def get_num_weights():
    return (input_size * hidden_size) + hidden_size + (hidden_size * output_size) + output_size

GENOME_LENGTH = get_num_weights()

# Activation functions
def relu(x): return np.maximum(0, x)

def softmax(x):
    try:
        # Shift values by the maximum of each row for numerical stability
        shifted_x = x - np.max(x, axis=1, keepdims=True)
        exp_x = np.exp(shifted_x)
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)
    except:
        # Handle edge cases (like single sample input)
        if x.ndim == 1:
            shifted_x = x - np.max(x)
            exp_x = np.exp(shifted_x)
            return exp_x / np.sum(exp_x)
        raise

def decode_weights(genome):
    idx = 0
    W1 = genome[idx:idx + input_size * hidden_size].reshape((input_size, hidden_size))
    idx += input_size * hidden_size
    b1 = genome[idx:idx + hidden_size]
    idx += hidden_size
    W2 = genome[idx:idx + hidden_size * output_size].reshape((hidden_size, output_size))
    idx += hidden_size * output_size
    b2 = genome[idx:idx + output_size]
    return W1, b1, W2, b2

def forward_pass(genome, X):
    W1, b1, W2, b2 = decode_weights(genome)
    z1 = relu(np.dot(X, W1) + b1)
    output = softmax(np.dot(z1, W2) + b2)
    return output

# Evaluate model on data with comprehensive metrics
def evaluate(genome, X, y):
    preds = forward_pass(genome, X)
    pred_labels = np.argmax(preds, axis=1)
    true_labels = np.argmax(y, axis=1)

    # Check if all classes are being predicted
    unique_preds = set(pred_labels)
    unique_true = set(true_labels)

    if len(unique_preds) < len(unique_true):
        print(f"Warning: Model only predicting {len(unique_preds)} out of {len(unique_true)} classes")

    acc = accuracy_score(true_labels, pred_labels)

    # Use zero_division parameter to avoid warnings
    precision, recall, f1, _ = precision_recall_fscore_support(
        true_labels, pred_labels, average='weighted', zero_division=0
    )

    return {
        'accuracy': acc,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'predictions': pred_labels,
        'true_labels': true_labels
    }

def fitness(genome):
    eval_results = evaluate(genome, X_train, y_train)
    return eval_results['accuracy']

# ACO parameters
NUM_ANTS = 30
ITERATIONS = 100
PHEROMONE_INIT = 1.0
EVAPORATION_RATE = 0.1
ALPHA = 1.0  # influence of pheromone
BETA = 2.0   # influence of heuristic (optional)
CONVERGENCE_THRESHOLD = 0.01  # Define convergence as <1% improvement over 10 iterations

# Performance metrics tracking
train_accuracy_history = []
val_accuracy_history = []
best_fitness_history = []
avg_fitness_history = []
execution_times = []
convergence_iteration = None

# Initialize pheromone levels for each gene position
pheromone = np.ones((GENOME_LENGTH,)) * PHEROMONE_INIT

# Main ACO loop
best_genome = None
best_score = 0

# Start timer for total execution
start_time_total = time.time()

for itr in range(ITERATIONS):
    # Start timer for this iteration
    start_time_iter = time.time()

    ant_solutions = []
    ant_scores = []

    for _ in range(NUM_ANTS):
        # Each ant builds a solution based on pheromones
        genome = np.zeros(GENOME_LENGTH)
        for i in range(GENOME_LENGTH):
            # Sample from normal distribution biased by pheromone
            bias = pheromone[i]
            genome[i] = np.random.normal(loc=0, scale=1) * (1 + bias)

        score = fitness(genome)
        ant_solutions.append(genome)
        ant_scores.append(score)

        if score > best_score:
            best_score = score
            best_genome = genome

    # Update pheromones
    pheromone = (1 - EVAPORATION_RATE) * pheromone  # evaporate
    best_ants = np.argsort(ant_scores)[-5:]  # top 5 ants
    for idx in best_ants:
        for i in range(GENOME_LENGTH):
            pheromone[i] += ALPHA * ant_solutions[idx][i] * ant_scores[idx]

    # Record metrics for this iteration
    current_best_idx = np.argmax(ant_scores)
    current_best_genome = ant_solutions[current_best_idx]

    # Record best and average fitness for this iteration
    current_best_fitness = ant_scores[current_best_idx]
    avg_fitness = sum(ant_scores) / len(ant_scores)
    best_fitness_history.append(current_best_fitness)
    avg_fitness_history.append(avg_fitness)

    # Evaluate on train and validation sets
    train_eval = evaluate(best_genome, X_train, y_train)
    val_eval = evaluate(best_genome, X_val, y_val)

    train_accuracy_history.append(train_eval['accuracy'])
    val_accuracy_history.append(val_eval['accuracy'])

    # Record execution time for this iteration
    end_time_iter = time.time()
    execution_times.append(end_time_iter - start_time_iter)

    # Check for convergence
    if itr >= 10 and convergence_iteration is None:
        improvement = (best_fitness_history[-1] - best_fitness_history[-10]) / best_fitness_history[-10] if best_fitness_history[-10] > 0 else float('inf')
        if improvement < CONVERGENCE_THRESHOLD:
            convergence_iteration = itr
            print(f"Converged at iteration {itr} with accuracy {best_score:.4f}")

    # Print progress and metrics
    if itr % 10 == 0 or itr == ITERATIONS - 1:
        print(f"Iteration {itr} - Train Acc: {train_eval['accuracy']:.4f}, Val Acc: {val_eval['accuracy']:.4f}, Time: {execution_times[-1]:.4f}s")
        # Print overfitting metric (train acc - val acc)
        overfitting_gap = train_eval['accuracy'] - val_eval['accuracy']
        print(f"Overfitting gap: {overfitting_gap:.4f}")
        print(f"Best Accuracy so far: {best_score:.4f}")

# End timer for total execution
end_time_total = time.time()
total_time = end_time_total - start_time_total

# Final evaluation on all datasets
final_train_eval = evaluate(best_genome, X_train, y_train)
final_val_eval = evaluate(best_genome, X_val, y_val)
final_test_eval = evaluate(best_genome, X_test, y_test)

# Print final results
print("\n======= FINAL RESULTS =======")
print(f"Total execution time: {total_time:.4f} seconds")
print(f"Average time per iteration: {sum(execution_times)/len(execution_times):.4f} seconds")

if convergence_iteration:
    print(f"Converged at iteration: {convergence_iteration}")
else:
    print("Did not converge within threshold")

print(f"\nFinal Train Accuracy: {final_train_eval['accuracy']:.4f}")
print(f"Final Validation Accuracy: {final_val_eval['accuracy']:.4f}")
print(f"Final Test Accuracy: {final_test_eval['accuracy']:.4f}")
print(f"Precision: {final_test_eval['precision']:.4f}")
print(f"Recall: {final_test_eval['recall']:.4f}")
print(f"F1 Score: {final_test_eval['f1']:.4f}")

# Overfitting analysis
train_val_gap = final_train_eval['accuracy'] - final_val_eval['accuracy']
print(f"\nOverfitting Analysis:")
print(f"Train-Validation accuracy gap: {train_val_gap:.4f}")

if train_val_gap > 0.1:
    print("WARNING: Possible overfitting detected (train accuracy significantly higher than validation)")
elif final_train_eval['accuracy'] < 0.7:
    print("WARNING: Possible underfitting detected (train accuracy is low)")
else:
    print("Model fit appears reasonable")

# Plot metrics
plt.figure(figsize=(15, 10))

# Plot 1: Accuracy over iterations
plt.subplot(2, 2, 1)
plt.plot(train_accuracy_history, label='Train Accuracy')
plt.plot(val_accuracy_history, label='Validation Accuracy')
plt.title('Accuracy over Iterations')
plt.xlabel('Iteration')
plt.ylabel('Accuracy')
plt.legend()

# Plot 2: Convergence (best and average fitness)
plt.subplot(2, 2, 2)
plt.plot(best_fitness_history, label='Best Fitness')
plt.plot(avg_fitness_history, label='Average Fitness')
plt.title('Fitness Convergence')
plt.xlabel('Iteration')
plt.ylabel('Fitness')
plt.legend()

# Plot 3: Execution time per iteration
plt.subplot(2, 2, 3)
plt.plot(execution_times)
plt.title('Execution Time per Iteration')
plt.xlabel('Iteration')
plt.ylabel('Time (seconds)')

# Plot 4: Overfitting gap over iterations
plt.subplot(2, 2, 4)
overfitting_gaps = [t - v for t, v in zip(train_accuracy_history, val_accuracy_history)]
plt.plot(overfitting_gaps)
plt.axhline(y=0.1, color='r', linestyle='--', label='Overfitting Threshold')
plt.title('Overfitting Gap (Train - Validation)')
plt.xlabel('Iteration')
plt.ylabel('Accuracy Gap')

plt.tight_layout()
plt.savefig('aco_performance_metrics.png')
plt.show()

# Confusion matrix for the test set
cm = confusion_matrix(final_test_eval['true_labels'], final_test_eval['predictions'])

# Plot confusion matrix
plt.figure(figsize=(8, 6))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()
tick_marks = np.arange(len(iris.target_names))
plt.xticks(tick_marks, iris.target_names, rotation=45)
plt.yticks(tick_marks, iris.target_names)
plt.xlabel('Predicted label')
plt.ylabel('True label')

# Add text annotations to confusion matrix cells
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, cm[i, j], horizontalalignment="center", color="white" if cm[i, j] > cm.max() / 2 else "black")

plt.tight_layout()
plt.savefig('aco_confusion_matrix.png')
plt.show()

# Check for class imbalance in training data
train_class_dist = np.sum(y_train, axis=0)
print("\nClass distribution in training data:")
for i, count in enumerate(train_class_dist):
    print(f"Class {iris.target_names[i]}: {count} samples")

# Calculate pheromone distribution statistics to analyze ACO behavior
pheromone_stats = {
    'mean': np.mean(pheromone),
    'std': np.std(pheromone),
    'min': np.min(pheromone),
    'max': np.max(pheromone)
}

print("\nFinal pheromone statistics:")
print(f"Mean: {pheromone_stats['mean']:.4f}")
print(f"Standard deviation: {pheromone_stats['std']:.4f}")
print(f"Min: {pheromone_stats['min']:.4f}")
print(f"Max: {pheromone_stats['max']:.4f}")

# Plot pheromone distribution
plt.figure(figsize=(10, 6))
plt.hist(pheromone, bins=30)
plt.title('Final Pheromone Distribution')
plt.xlabel('Pheromone Value')
plt.ylabel('Frequency')
plt.savefig('pheromone_distribution.png')
plt.show()